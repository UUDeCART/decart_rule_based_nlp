{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis (2)\n",
    "\n",
    "From the previous notebook, you have seen that pyConTextNLP helped to improved the precision by exclude irrelevant annotations based on the modifiers. \n",
    "\n",
    "This notebook will continue our previous error analyses by including both false negative errors and false positive ones, through step by step demonstration of how to locate and reduce the errors.\n",
    "\n",
    "## 1. Locate the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let import some packages\n",
    "import os\n",
    "import pyConTextNLP\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import radnlp.view as rview\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, HTML, Image\n",
    "import ipywidgets\n",
    "# and also our utilities for this class\n",
    "\n",
    "from nlp_pneumonia_utils import read_doc_annotations\n",
    "from nlp_pneumonia_utils import mark_document_with_html\n",
    "from nlp_pneumonia_utils import DocumentClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember yesterday, in [06_NLP_ErrorAnalysis1](06_NLP_ErrorAnalysis1.ipynb#cell1), we created a function called *\"list_false_negatives.\"* Now we will extend this function to return both **false negative** and **false positive** document names at the same time: *\"list_errors.\"* Additionally, we will also integrate the *\"calculate_prediction_metrics\"* function inside *\"list_errors\"*, so that we can get the metrics without re-run the pyConText over the documents again.\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_errors(gold_docs, prediction_function, print_prediction_metrics=False):\n",
    "    fn_docs=[]\n",
    "    fp_docs=[]\n",
    "    gold_labels = [x.positive_label for x in gold_docs.values()]\n",
    "    pred_labels = []\n",
    "    for doc_name, gold_doc in gold_docs.items():\n",
    "        gold_label=gold_doc.positive_label;\n",
    "        pred_label = prediction_function(gold_doc.text,{'indicate_pneumonia'},doc_name)\n",
    "        pred_labels.append(pred_label)\n",
    "#       differentiate false positive and false negative error\n",
    "        if gold_label==0 and pred_label==1:\n",
    "            fp_docs.append(doc_name)\n",
    "        elif gold_label==1 and pred_label==0:\n",
    "            fn_docs.append(doc_name)\n",
    "    if(print_prediction_metrics):\n",
    "        precision = sklearn.metrics.precision_score(gold_labels, pred_labels)\n",
    "        recall = sklearn.metrics.recall_score(gold_labels, pred_labels)\n",
    "        f1 = sklearn.metrics.f1_score(gold_labels, pred_labels)\n",
    "        # let's use Pandas to make a confusion matrix for us\n",
    "        confusion_matrix_df = pd.crosstab(pd.Series(gold_labels, name='Actual'),\n",
    "                                          pd.Series(pred_labels, name='Predicted'))\n",
    "\n",
    "        print('Precision : {0}'.format(precision))\n",
    "        print('Recall :    {0}'.format(recall))\n",
    "        print('F1:         {0}'.format(f1))\n",
    "\n",
    "        print('Confusion Matrix : ')\n",
    "        print(confusion_matrix_df)\n",
    "    return fn_docs,fp_docs   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we restore what we got from [10_NLP_DocumentClassification.ipynb](10_NLP_DocumentClassification.ipynb):<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the training documents and annotations\n",
    "annotated_doc_map = read_doc_annotations('data/training_v2.zip')\n",
    "\n",
    "#Here we initiate our DocumentClassifier directly through rule files:\n",
    "#Change the file names if you use different files \n",
    "docClassifier = DocumentClassifier('KB/classifierRules.csv', False,'KB/pneumonia_modifiers.tsv','KB/pneumonia_targets.tsv') \n",
    "docClassifier.reset_saved_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the corpus using docClassifier to return errors\n",
    "current_false_negatives,current_false_positives=list_errors(annotated_doc_map, docClassifier.predict,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display errors\n",
    "Now we put everything together to display errors. Let's try false positive first:<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy the listing snippets function from 06_NLP_ErrorAnalysis1\n",
    "def snippets_markup(annotated_doc_map):\n",
    "    html = [\"<html>\",\"<table width=100% >\",\n",
    "            \"<col style=\\\"width:25%\\\"><col style=\\\"width:75%\\\">\"\n",
    "            \"<tr><th style=\\\"text-align:center\\\">document name</th><th style=\\\"text-align:center\\\">Snippets</th>\"]\n",
    "    for doc_name, anno_doc in annotated_doc_map.items():\n",
    "        html.extend(snippet_markup(doc_name,anno_doc))\n",
    "    html.append(\"</table>\")\n",
    "    html.append(\"</html>\")\n",
    "    return ''.join(html) \n",
    "\n",
    "\n",
    "def snippet_markup(doc_name,anno_doc):\n",
    "    from pyConTextNLP.display.html import __sort_by_span\n",
    "    from pyConTextNLP.display.html import __insert_color\n",
    "    html=[]\n",
    "    color= 'blue'    \n",
    "    window_size=50    \n",
    "    html.append(\"<tr>\")\n",
    "    html.append(\"<td style=\\\"text-align:left\\\">{0}</td>\".format(doc_name))\n",
    "    html.append(\"<td></td>\")\n",
    "    html.append(\"</tr>\")\n",
    "    for anno in anno_doc.annotations:\n",
    "        if anno.type == 'SPAN_POSITIVE_PNEUMONIA_EVIDENCE':\n",
    "#           make sure the our snippet will be cut inside the text boundary\n",
    "            begin=anno.start_index-window_size\n",
    "            end=anno.end_index+window_size\n",
    "            begin=begin if begin>0 else 0\n",
    "            end=end if end<len(anno_doc.text) else len(anno_doc.text)    \n",
    "#           render a highlighted snippet\n",
    "            cell=__insert_color(anno_doc.text[begin:end],[anno.start_index-begin,anno.end_index-end],color)\n",
    "#           add the snippet into table\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\"<td></td>\")\n",
    "            html.append(\"<td style=\\\"text-align:left\\\">{0}</td>\".format(cell))\n",
    "            html.append(\"</tr>\") \n",
    "    return html\n",
    "\n",
    "# Let's tweak the pyConText markup display function from 09_NLP_pneumonia_pyConText_targets_and_modifiers\n",
    "# This function let's us view saved markups in docClassifier\n",
    "def view_pycontext_graph(saved_markups, colors):\n",
    "    @interact(i=ipywidgets.IntSlider(min=0, max=len(saved_markups)-1))\n",
    "    def _view_markup(i):\n",
    "        markup = saved_markups[i]\n",
    "        ag=nx.nx_pydot.to_pydot(rview.documentgraph_to_viewgraph(markup.getDocumentGraph()))\n",
    "        ag.write_png(\"tmp.png\")\n",
    "        display(Image(\"tmp.png\"))        \n",
    "        report_html = mark_document_with_html(markup, colors, default_color=\"black\")        \n",
    "        display(HTML(report_html))\n",
    "        \n",
    "colors = {\n",
    "    \"evidence_of_pneumonia\": \"orange\",\n",
    "    \"definite_negated_existence\": \"red\",\n",
    "    \"probable_negated_existence\": \"indianred\",\n",
    "    \"ambivalent_existence\": \"orange\",\n",
    "    \"probable_existence\": \"forestgreen\",\n",
    "    \"definite_existence\": \"green\",\n",
    "    \"historical\": \"goldenrod\",\n",
    "    \"indication\": \"pink\",\n",
    "    \"acute\": \"golden\"\n",
    "}        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Display false negatives\n",
    "Now we can display the **false negatives** with expert annotations.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_docs=dict((k, v) for k, v in annotated_doc_map.items() if k in current_false_negatives)\n",
    "display(HTML(snippets_markup(fn_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### * Display false positives\n",
    "Then we can display the **false positives** with pyConText markups.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_docs=list(v for k,v in docClassifier.saved_markups_map.items() if k in current_false_positives)\n",
    "view_pycontext_graph(fp_docs,colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Rethink the causes of the false negatives\n",
    "Does the false negatives are all caused by missed keywords?<br/><br/>\n",
    "You may want to review these pyConText markups in **false negative** documents:<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_docs=list(v for k,v in docClassifier.saved_markups_map.items() if k in current_false_negatives)\n",
    "view_pycontext_graph(fp_docs,colors)\n",
    "# Because \"saved_markups_map\" in \"docClassifier\" only saves the documents that have at least one annotation. \n",
    "# Thus the false negatives caused by missing keywords (no annotations) will not be saved in it,\n",
    "# and only pyConText caused false negatives will be displayed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Now what?<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
