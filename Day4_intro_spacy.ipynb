{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from IPython.display import SVG\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to spaCy\n",
    "We'll finish up our course by looking at `spaCy`, a great Python library for working with natural language processing. While most of the tools we've used so far have been rule-based, spaCy consists mostly of **statistical NLP** models. In statistical models, a large corpus of text is processed and mathematical methods are used to identify patterns in the corpus. This process is called **training**. Once a model has been trained, we can use it to analyze new text. \n",
    "\n",
    "spaCy comes with several pre-trained models, meaning that we can quickly load a model which has been trained on large amounts of data. This way, we can take advantage of work which has already been done by spaCy developers and focus on our own NLP tasks. In these notebooks, we'll see how combining spaCy's statistical models with rule-based systems offers a powerful way to process and analyze text.\n",
    "\n",
    "## What we'll do today\n",
    "\n",
    "We'll start by looking at the basic usage of spaCy. Next, we'll focus on specific NLP task, **named entity recognition (NER)**, and see how this works in spaCy, as well as some of the limitations with clinical data. Some of these limitations can be addressed by writing our own rules for concept extraction, and we'll practice that with some clinical texts. We'll then go a little deeper into how spaCy's models are implemented and how we can modify them. Finally, we'll end the day by spaCy models which were designed specifically for use in the biomedical domain.\n",
    "\n",
    "# spaCy documentation\n",
    "\n",
    "spaCy has great documentation. As we're going along today, try browsing through their documentation to find examples and instructions. Start by opening up these two pages and navigating through the documentation:\n",
    "\n",
    "[Basic spaCy usage](https://spacy.io/usage/models)\n",
    "\n",
    "[API documentation](https://spacy.io/api)\n",
    "\n",
    "spaCy also has a really good, free online class. If you want to dig deeper into spaCy after this class, it's a great resource for using this library:\n",
    "https://course.spacy.io/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage of spaCy\n",
    "\n",
    "\n",
    "In this notebook, we'll look at the basic fundamentals of spaCy:\n",
    "- Main classes in spaCy\n",
    "- Linguistic attributes available as part of default text processing\n",
    "- Coding exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use spaCy\n",
    "At a high-level, here are the steps for using spaCy:\n",
    "- Start by loading a pre-trained NLP model\n",
    "- Process a string of text with the model\n",
    "- Use the attributes in our processed documents for downstream NLP tasks like NER or document classification\n",
    "\n",
    "For example, here's a very short example of how this works. For the sake of demonstration, we'll use this snippet of a recent, exciting news article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load a pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a string of text with the model\n",
    "text = \"\"\"Taco Bell’s latest marketing venture, a pop-up hotel, opened at 10 a.m. Pacific Time Thursday. \n",
    "The rooms sold out within two minutes.\n",
    "The resort has been called “The Bell: A Taco Bell Hotel and Resort.” It’s located in Palm Springs, California.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the attributes in our processed documents for downstream NLP tasks\n",
    "# Here, we'll visualize the entities in this text identified through NER\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive a little deeper into how spaCy is structured and what we have to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy architecture\n",
    "The following diagram from spaCy's API documentation shows a basic overview of spaCy's architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG('./images/spacy_architecture.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of information in this diagram, so we'll start small by focusing on these 4 spaCy classes:\n",
    "- `Language`: The NLP model used to process text\n",
    "- `Doc`: A sequence of text which has been processed by a `Language` object\n",
    "- `Token`: A single word or symbol in a Doc\n",
    "- `Span`: A slice from a Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Language` class\n",
    "The `nlp` object in spaCy is the linguistic model which will be used for processing text. We instantiate a `Language` class by providing the name of a pre-trained model which we wish to use. We typically name this object `nlp`, and this will be our primary entry point.\n",
    "\n",
    "\n",
    "Statistical, pre-trained, explain more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nlp` model we instantiated above is a **small** (\"sm\"), **English** (\"en\")-language model trained on **web** (\"web\") data, but there are currently 16 different models from 9 different languages. See the [spaCy documentation](https://spacy.io/usage/models) for more information on each of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents, spans and tokens\n",
    "The `nlp` object is what we'll be using to process text. The next few classes represent the output of our NLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Doc` class\n",
    "The `doc` object represents a single document of text. To create a `doc` object, we call `nlp` on a string of text. This runs that text through a spaCy pipeline, which we'll learn more about in a future notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Taco Bell’s latest marketing venture, a pop-up hotel, opened at 10 a.m. Pacific Time Thursday.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens and Spans\n",
    "A `Token` is a single word, symbol, or whitespace in a `doc`. When we create a `doc` object, the text broken up into individual tokens. This is called **\"tokenization\"**.\n",
    "\n",
    "**Discussion**: Look at the tokens generated from this text snippet. What can you say about the tokenization method? Is it as simple as splitting up into words every time we reach a whitespace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Span` is a slice of a document, or a consecutive sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Attributes\n",
    "Because spaCy comes with pre-trained linguistic models, when we call `nlp` on a text we have access to a number of linguistic attributes in the `doc` or `token` objects:\n",
    "\n",
    "- Part-of-speech (POS) tagging\n",
    "- Morphology\n",
    "- Dependency Parsing\n",
    "- Named entity recognition\n",
    "- Sentence splitting\n",
    "- Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "Parts of speech are categories of words. For example, \"nouns\", \"verbs\", and \"adjectives\" are all examples of parts of speech. Assigning parts of speech to words is useful for downstream NLP texts such as word sense disambiguation and named entity recognition.\n",
    "\n",
    "**Discussion**: What to the POS tags below mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Token -> POS\\n\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphology\n",
    "The morphology of a word refers to the form of a word. For example, \"eat\", \"eats\", and \"ate\" are all different inflections of the word \"eat\". We would say that \"eat\" is the **lemma** of all of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Token -> Lemma\\n\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "In dependency parsing, we analyze the structure of a sentence. We won't spend too much time on this, but here is a nice visualization of dependency parse looks like. Take a minute to look at the arrows between words and try to figure out what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The cat sat on the green mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other attributes\n",
    "Look at spaCy's [Token class documentation](https://spacy.io/api/token) for a full list of additional attributes available for each token in a document. We'll print out a few more.\n",
    "\n",
    "**Discussion**: How can these attributes be useful in downstream NLP tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Bitcoin rose 7.9% to $11,899 as of 11:53 a.m. in New York on Monday. \"\n",
    "    \"https://www.bloomberg.com/news/articles/2019-07-08/bitcoin-breakout-may-be-ahead-as-technicals-show-rally-extension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"shape\" of the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0])\n",
    "print(doc[0].shape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector of a token (we'll learn more about this in the upcoming [statistical NLP class](https://datascience4health.bmi.utah.edu/statistical-nlp/) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the token resembles a:\n",
    "- Number\n",
    "- Punctuation\n",
    "- Currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[2])\n",
    "print(doc[2].like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[5])\n",
    "print(doc[5].is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[-1])\n",
    "print(doc[-1].like_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Exercises and Discussions\n",
    "Now that we've seen some examples of what we can do with spaCy, let's practice with some coding exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Ambiguity** \n",
    "\n",
    "Consider these two sentences and look at the word \"duck\". Consider these questions and discuss them with a group:\n",
    "- Are the two tokens \"duck\" in these two sentences identical? What does that tell us about a \"token\" vs. a normal string?\n",
    "- When two words are spelled the same but have different meanings, they are \"ambiguous\". Using spaCy, what are some ways we we could \"disambiguate\" the word \"duck\" in these two sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"The duck swam gently down the river.\")\n",
    "doc2 = nlp(\"He had to duck as he came through the door.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duck1 = doc1[1]\n",
    "duck2 = doc2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duck1 == duck2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Count POS tags** \n",
    "\n",
    "Write a function that takes a Doc and returns a count of the number of each POS tag in that doc.\n",
    "\n",
    "*Bonus*: Write a second function that plots a bar graph of these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def count_pos_tags(doc):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # d = defaultdict(...)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"The highly vivacious green paint is giving me a terrible, mind-splitting headache.\")\n",
    "doc2 = nlp(\"My favorite activites are swimming, biking, reading, eating, and watching trashy shows on Netflix.\")\n",
    "doc3 = nlp(\"On Tuesday, the 28 EU leaders chose Ursula von der Leyen, an ally of German Chancellor Angela Merkel, \"\n",
    "            \"to replace Jean-Claude Juncker at the helm of the Commission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = (doc1, doc2, doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(count_pos_tags(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Sort by number of words with a certain POS tag**\n",
    "\n",
    "Write a function that takes a list of Docs and a POS tag. Return a new list which is sorted in **descending order** by the number of tokens which have that POS tag. Use a default value of 'PROPN' for the argument `pos_tag`. Test this out using the list `docs` from the last exercise.\n",
    "\n",
    "**Hint**: Use the built-in [sorted function](https://www.geeksforgeeks.org/sorted-function-python/) to sort the list, and use a lambda function to define the key which we should sort by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_pos(docs, pos_tag):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # srtd_docs = ... \n",
    "    return srtd_docs\n",
    "    \n",
    "def count_pos_tag(doc, pos_tag):\n",
    "    n = 0\n",
    "    for token in doc:\n",
    "        # Your code here\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by which documents have the most proper nouns\n",
    "sort_by_pos(docs, 'PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by which documents have the most verbs\n",
    "sort_by_pos(docs, 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by which documents have the most adjectives\n",
    "sort_by_pos(docs, 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
