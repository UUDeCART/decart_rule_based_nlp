{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import codecs\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        self.start_index = -1\n",
    "        self.end_index = -1\n",
    "        self.type = ''\n",
    "        self.spanned_text = ''\n",
    "        \n",
    "    # adding this so that pyConText's HTML markup can work seamlessly\n",
    "    def getSpan(self):\n",
    "        return (self.start_index, self.end_index)\n",
    "    \n",
    "    def getCategory(self):\n",
    "        # pyConText graph objects actually expect a list here\n",
    "        return [self.type]\n",
    "\n",
    "class AnnotatedDocument(object):\n",
    "    def __init__(self):\n",
    "        self.text = ''\n",
    "        self.annotations = []\n",
    "        self.positive_label = -1\n",
    "        \n",
    "def read_brat_annotations(lines):\n",
    "    annotations = []\n",
    "    # BRAT FORMAT is:\n",
    "    # NUMBER[TAB]TYPE[SPACE]START_INDEX[SPACE]END_INDEX[SPACE]SPANNED_TEXT\n",
    "    for line in lines:\n",
    "        line = str(line)\n",
    "        tab_tokens = line.split('\\t')\n",
    "        space_tokens = tab_tokens[1].split()\n",
    "        anno = Annotation()\n",
    "        anno.spanned_text = tab_tokens[-1]\n",
    "        anno.type = space_tokens[0]\n",
    "        anno.start_index = int(space_tokens[1])\n",
    "        anno.end_index = int(space_tokens[2])\n",
    "        annotations.append(anno)\n",
    "    return annotations\n",
    "        \n",
    "def read_annotations(archive_file, force_redownload = False):    \n",
    "    \n",
    "    annotated_doc_map = read_doc_annotations(archive_file, force_redownload)\n",
    "                    \n",
    "    return list(annotated_doc_map.values())\n",
    "\n",
    "def read_doc_annotations(archive_file, force_redownload = False):\n",
    "    print('Reading annotations from file : ' + archive_file)\n",
    "    \n",
    "    if 'http' in archive_file:\n",
    "        if force_redownload or not os.path.isfile(filename):\n",
    "            print('Downloading remote file : '+ archive_file)\n",
    "            urllib.request.urlretrieve(archive_file, filename)\n",
    "            filename = archive_file.split('/')[-1]\n",
    "    else:\n",
    "        filename = archive_file\n",
    "    \n",
    "    annotated_doc_map = {}\n",
    "    \n",
    "    print('Opening local file : ' + filename)\n",
    "    z = zipfile.ZipFile(filename, \"r\")\n",
    "    zinfo = z.namelist()\n",
    "    for name in zinfo:\n",
    "        if name.endswith('.txt') or name.endswith('.ann'):\n",
    "            basename = name.split('.')[0]\n",
    "            if basename not in annotated_doc_map:\n",
    "                annotated_doc_map[basename] = AnnotatedDocument()\n",
    "            anno_doc = annotated_doc_map[basename]\n",
    "            # handle text and BRAT annotation files (.ann) differently\n",
    "            if name.endswith('.txt'):\n",
    "                with z.open(name) as f1:\n",
    "                    anno_doc.text = f1.read().decode('utf8')\n",
    "            else:\n",
    "                with z.open(name) as f1:\n",
    "                    # handle this as utf8 or we get back byte arrays\n",
    "                    anno_doc.annotations = read_brat_annotations(codecs.iterdecode(f1, 'utf8'))\n",
    "                    \n",
    "    # now let's finally assign a 0 or 1 to each document based on whether we see our expected type for the pneumonia label\n",
    "    for key, anno_doc in annotated_doc_map.items():\n",
    "        annos = anno_doc.annotations\n",
    "        anno_doc.positive_label = 0\n",
    "        for anno in annos:\n",
    "            # NOTE : This \"positive_label\" relates to positive/possible cases of pneumonia\n",
    "            if anno.type == 'DOCUMENT_PNEUMONIA_YES':\n",
    "                anno_doc.positive_label = 1\n",
    "                    \n",
    "    return annotated_doc_map\n",
    "\n",
    "def calculate_prediction_metrics(gold_docs, prediction_function):\n",
    "    gold_labels = [x.positive_label for x in gold_docs]\n",
    "    pred_labels = []\n",
    "    for gold_doc in gold_docs:\n",
    "        pred_label = prediction_function(gold_doc.text)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "    # now let's use scikit-learn to compute some metrics\n",
    "    precision = sklearn.metrics.precision_score(gold_labels, pred_labels)\n",
    "    recall = sklearn.metrics.recall_score(gold_labels, pred_labels)\n",
    "    f1 = sklearn.metrics.f1_score(gold_labels, pred_labels)\n",
    "    # let's use Pandas to make a confusion matrix for us\n",
    "    confusion_matrix_df = pd.crosstab(pd.Series(gold_labels, name = 'Actual'), \n",
    "                                      pd.Series(pred_labels, name = 'Predicted'))\n",
    "    \n",
    "    print('Precision : {0}'.format(precision))\n",
    "    print('Recall : {0}'.format(recall))\n",
    "    print('F1: {0}'.format(f1))\n",
    "    \n",
    "    print('Confusion Matrix : ')\n",
    "    print(confusion_matrix_df)\n",
    "    \n",
    "# First thing, let's load our training set\n",
    "annotated_doc_map = read_doc_annotations('data/training.zip')\n",
    "#annotated_doc_map = read_doc_annotations('pneumonia_brat_full_set1.zip')\n",
    "# let's also use a simple list of documents as well as this map\n",
    "annotated_docs = list(annotated_doc_map.values())\n",
    "print('Total Annotated Documents : {0}'.format(len(annotated_docs)))\n",
    "\n",
    "total_positives = 0\n",
    "for anno_doc in annotated_docs:\n",
    "    if anno_doc.positive_label:\n",
    "        total_positives += 1\n",
    "    \n",
    "print('Total Positive Pneumonia Documents : {0}'.format(total_positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions to highlight annotations from BRAT\n",
    "def mark_text(txt,nodes,colors = {\"name\":\"red\",\"pet\":\"blue\"},default_color=\"black\"):\n",
    "    from pyConTextNLP.display.html import __insert_color\n",
    "    # this function had to be copied and modified from pyConTextNLP.display.html.mark_text \n",
    "    # so that the default_color could be passed through\n",
    "    if not nodes:\n",
    "        return txt\n",
    "    else:\n",
    "        n = nodes.pop(-1)\n",
    "        return mark_text(__insert_color(txt,\n",
    "                                        n.getSpan(),\n",
    "                                        colors.get(n.getCategory()[0],default_color)),\n",
    "                         nodes,\n",
    "                         colors=colors,\n",
    "                         # this was not being passed through \n",
    "                        default_color = default_color)\n",
    "    \n",
    "def pneumonia_html_markup(anno_doc):\n",
    "    from pyConTextNLP.display.html import __sort_by_span\n",
    "    # this bit mimics 'mark_document_with_html' from pyConTextNLP.display.html\n",
    "    colors = {}\n",
    "    colors['DOCUMENT_PNEUMONIA_YES'] = 'red'\n",
    "    colors['DOCUMENT_PNEUMONIA_NO'] = 'green'\n",
    "    colors['SPAN_POSITIVE_PNEUMONIA_EVIDENCE'] = 'red'\n",
    "    default_color = 'red'\n",
    "    html = \"\"\"<p> {0} </p>\"\"\".format(\" \".join([mark_text(anno_doc.text,\n",
    "                                                 __sort_by_span(anno_doc.annotations),\n",
    "                                                 colors=colors,\n",
    "                                                 default_color=default_color)]))\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's find the document with the most annotations\n",
    "most_annotated_doc = None\n",
    "for anno_doc in annotated_docs:\n",
    "    \n",
    "    #for anno in anno_doc.annotations:\n",
    "    #    print(anno.getCategory())\n",
    "    \n",
    "    if most_annotated_doc is None or len(anno_doc.annotations) > len(most_annotated_doc.annotations):\n",
    "        most_annotated_doc = anno_doc\n",
    "        print('Most Annotations so far : {}'.format(len(most_annotated_doc.annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's display one of our documents in HTML\n",
    "display(HTML(pneumonia_html_markup(most_annotated_doc).replace('\\n', '<br>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's first illustrate a naive baseline by always prediction NO pneumonia (i.e. 0)\n",
    "def naive_negative_pneumonia_prediction(text):\n",
    "    return 0\n",
    "    \n",
    "print('Predicting and validating the naive baseline of always predicting NO')\n",
    "calculate_prediction_metrics(annotated_docs, naive_negative_pneumonia_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's first illustrate a naive baseline by always prediction NO pneumonia (i.e. 0)\n",
    "def naive_positive_pneumonia_prediction(text):\n",
    "    return 1\n",
    "    \n",
    "print('Predicting and validating the naive baseline of always predicting YES')\n",
    "calculate_prediction_metrics(annotated_docs, naive_positive_pneumonia_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's try a very naive simulated baseline to assign positive Pneumonia anytime the work \"pneumonia\" appears in a document\n",
    "def naive_pneumonia_keyword_prediction(text):\n",
    "    if 'pneumonia' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "print('Predicting and validating the naive PNEUMONIA keyword baseline')\n",
    "calculate_prediction_metrics(annotated_docs, naive_pneumonia_keyword_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
