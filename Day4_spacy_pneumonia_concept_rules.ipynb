{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this notebook is to demonstrate how we can set up rules for concepts related to pneumonia using powerful matching in spacy.\n",
    "## Since this matcher uses tokens, but can also use regular expressions we can do most of what we have done so far in class (a la pyConTextNLP).\n",
    "## However, we can also leverage other text processing deeper than 'surface level' processing to take advantage of lemmas (stems), parts-of-speech, etc\n",
    "## Finally, this [Matcher](https://spacy.io/api/matcher) is not the fastest method if we have a very large vocabulary and we're only using surface forms (rather than token attributes).  If you are working with a large dictionary, spacy authors recommend the [PhraseMatcher](https://spacy.io/api/phrasematcher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start working with some simple documents.  Let's see what happens when we create a Matcher without adding rules to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our default English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matches(doc, matches):\n",
    "    print('Total matches found: {}'.format(len(matches)))\n",
    "    \n",
    "    # Iterate over the matches and print the span text\n",
    "    for match_id, start, end in matches:\n",
    "        print(\"Match found in text [from tokens {0} to {1}] : {2}\".format(start, end, doc[start : end].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "print('Total rules in matcher: {}'.format(len(matcher)))\n",
    "\n",
    "no_rules_doc = nlp(\"Patchy consolidation in the left lower lobe\")\n",
    "\n",
    "matches = matcher(no_rules_doc)\n",
    "\n",
    "print_matches(no_rules_doc, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we move ahead, let's point out some documentation that will be useful to know the valid pattern types we'll use below:\n",
    "Spacy Rule based Matching:\n",
    "\n",
    "https://spacy.io/usage/rule-based-matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern syntax\n",
    "## Each pattern is a list of 1 or more dictionaries, where each dictionary contains the key/value attributes that must be met for each token in the list to be considered a match.   Please note that since spacy has already run a tokenizer before we do matching, we do not need to worry about word boundaries (\\b in regular expressions) since this has already been handled.  Not all tokenizers are perfect, but this can mean one less thing we need to worry about\n",
    "\n",
    "For example this pattern matches a single word 'Hello' but only the exact case:\n",
    "\n",
    "[{\"TEXT\": \"Hello\"}]\n",
    "\n",
    "But this pattern pattern matches two words 'hello world' regardless of the case of any of the characters in these tokens:\n",
    "\n",
    "[{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are some of the \"token attributes\" that can be used to match:\n",
    "* 'TEXT' - The exact, case sensitive text of the token\n",
    "* 'LOWER' - The exact text of the token, but case insensitive\n",
    "* 'POS' - The part of speech of the token (e.g. )\n",
    "* 'TAG' - Same as above\n",
    "* 'LEMMA' - The lemma or \"stem\" of a world (e.g. the lemma of \"walking\" is \"walk\")\n",
    "* 'ENT_TYPE' - The entity type of the token (we will not be using this today, but this can be powerful if you have a good entity component in your pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So let's start out simply.  Update the pattern below using 'LOWER' to match both cases in this example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidation_case_text = 'Is consolidation the same as Consolidation and CONSOLIDATION?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidation_lower_pattern = [{\"CHANGE_ME\": \"CHANGE_ME\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"CONSOLIDATION\", None, consolidation_lower_pattern)\n",
    "\n",
    "print('Total rules in matcher: {}'.format(len(matcher)))\n",
    "\n",
    "case_doc = nlp(consolidation_case_text)\n",
    "\n",
    "matches = matcher(case_doc)\n",
    "\n",
    "print_matches(case_doc, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each token attribute, it must match a value.  This can either be a string like our examples above or a dictionary containing other matching criterialike a regular expression.  For example, this will match a single token whether it contains \"walk\" or \"talk\":\n",
    "\n",
    "[{\"TEXT\": {\"REGEX\": \"(walk|talk)\"}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So instead of a single string value, let's update regular expression criteria to match all of the forms of 'infiltrate' below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiltrate_variants_text = 'There exist not just one infiltrate but two infiltrates.  We have been infiltrated!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiltrate_lower_pattern = [{\"TEXT\": {\"CHANGE_ME\" : \"CHANGE_ME\"}}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"CONSOLIDATION\", None, infiltrate_lower_pattern)\n",
    "\n",
    "print('Total rules in matcher: {}'.format(len(matcher)))\n",
    "\n",
    "case_doc = nlp(infiltrate_variants_text)\n",
    "\n",
    "matches = matcher(case_doc)\n",
    "\n",
    "print_matches(case_doc, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's use some of the deeper power of spacy.  All of the information we've seen so far is at a surface level, meaning that we can see it in the literal text, but there is more to language like syntax, semantics, grammar, etc.  \n",
    "## Let's use Part-of-Speech ('POS') to match a single adjectives modifying another concept.  Note, some of the POS tags that might be useful include:\n",
    "* 'NOUN'\n",
    "* 'ADV'\n",
    "* 'ADJ'\n",
    "* 'PROPN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the pattern below to match consolidation and a single adjective before it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidation_doc = nlp(\"Patchy consolidation in the left upper lobe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern for adjective plus one or two nouns\n",
    "consolidation_pattern = [{\"CHANGE_ME\": \"CHANGE_ME\"}, {\"LOWER\": \"consolidation\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_CONSOLIDATION\", None, consolidation_pattern)\n",
    "\n",
    "print('Total rules in matcher: {}'.format(len(matcher)))\n",
    "\n",
    "matches = matcher(consolidation_doc)\n",
    "\n",
    "print_matches(consolidation_doc, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matcher also allows operators like quantifiers.  These are much like regular expression quantifiers.  To add these to your pattern, these serve as additional key/values in the dictionary for each token besides other token attributes you might want to match.  To use these operators, the key is 'OP'\n",
    "* '!'\tNegate the pattern, by requiring it to match exactly 0 times.\n",
    "* '?'\tMake the pattern optional, by allowing it to match 0 or 1 times.\n",
    "* '+'\tRequire the pattern to match 1 or more times.\n",
    "* '*'\tAllow the pattern to match zero or more times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now modify the pattern below to match 1 or more adjectives before the word lobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use OP to capture multiple ADJECTIVES\n",
    "lobe_pattern = [{\"POS\": \"ADJ\", \"CHANGE_ME\" : \"CHANGE_ME\"}, {\"LOWER\": \"lobe\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"LOBE\", None, lobe_pattern)\n",
    "\n",
    "print('Total rules in matcher: {}'.format(len(matcher)))\n",
    "\n",
    "matches = matcher(consolidation_doc)\n",
    "\n",
    "print_matches(consolidation_doc, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's experiment with another linguistic feature below the surface using LEMMA (i.e. stem, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiltrate_text = \"\"\"A pulmonary infiltrate is a substance denser than air,\n",
    "    such as pus, blood, or protein, which lingers within the parenchyma of the lungs.\n",
    "    Pulmonary infiltrates are associated with pneumonia, tuberculosis, and nocardiosis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiltrate_doc = nlp(infiltrate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiltrate_pattern = [{\"CHANGE_ME\": \"CHANGE_ME\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"INFILTRATE\", None, infiltrate_pattern)\n",
    "\n",
    "print('Total rules in matcher: {}'.format(len(matcher)))\n",
    "\n",
    "matches = matcher(infiltrate_doc)\n",
    "\n",
    "print_matches(infiltrate_doc, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK, let's go a bit further and combine what has been learned so far to match as much of you can of the following entities (or anatomical location) and any adjectives that modify them:\n",
    "* Consolidation\n",
    "* Infiltrate\n",
    "* Opacity\n",
    "* Any anatomical location (e.g. lobe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = 'patchy Consolidations present around the left lunG.'\n",
    "doc_2 = 'Hazy opacities in the left lung and streaky opaciTy in the right lower lobe'\n",
    "doc_3 = 'I saw a single alveolar infiltrate as well as bilateral infiltrates'\n",
    "doc_4 = 'Linear opacities from the upper left lobe down to the lower right lobe.'\n",
    "doc_5 = 'I was too busy studying anatomy like lobes, parenchyma, pleural cavity and diaphragm'\n",
    "\n",
    "example_texts = [doc_1, doc_2, doc_3, doc_4, doc_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# TODO -- add as many patterns as you would like by calling \n",
    "# my_matcher.add(\"my_match_id\", None, your_pattern_variable)\n",
    "\n",
    "for i, example_text in enumerate(example_texts):\n",
    "    print('Matching for Doc {0}: [{1}]'.format(i, example_text))\n",
    "    example_doc = nlp(example_text)\n",
    "    my_matches = my_matcher(example_doc)\n",
    "    print_matches(example_doc, my_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
